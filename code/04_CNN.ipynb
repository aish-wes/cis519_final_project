{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "## Data Preprocessing\n",
    "I will use the function \"extract_mel_spectrogram\" created in the first notebook to get the features and targets. Before constructing the model though, a few additional steps have to be taken:\n",
    "1. Values of the mel spectrograms should be scaled so that they are between 0 and 1 for computational efficiency.\n",
    "2. The data is currently 1000 rows of mel spectrograms that are 128 x 660. We need to reshape this to be 1000 rows of 128 x 660 x 1 to represent that there is a single color channel. If our image had three color channels, RGB, we would need this additional dimension to be 3.Â \n",
    "3. Target values have to be one-hot-encoded in order to be fed into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras import utils\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(directory):\n",
    "    '''\n",
    "    This function takes in a directory of audio files in .wav format, computes the\n",
    "    mel spectrogram for each audio file, reshapes them so that they are all the \n",
    "    same size, and stores them in a numpy array. \n",
    "    \n",
    "    It also creates a list of genre labels and maps them to numeric values.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (int): a directory of audio files in .wav format\n",
    "    \n",
    "    Returns:\n",
    "    X (array): array of mel spectrogram data from all audio files in the given\n",
    "    directory\n",
    "    y (array): array of the corresponding genre labels in numeric form\n",
    "    '''\n",
    "    \n",
    "    # Creating empty lists for mel spectrograms and labels\n",
    "    labels = []\n",
    "    mel_specs = []\n",
    "    \n",
    "    \n",
    "    # Looping through each file in the directory\n",
    "    for file in os.scandir(directory):\n",
    "        \n",
    "        # Loading in the audio file\n",
    "        y, sr = librosa.core.load(file)\n",
    "        \n",
    "        # Extracting the label and adding it to the list\n",
    "        label = str(file).split('.')[0][11:]\n",
    "        labels.append(label)\n",
    "        \n",
    "        # Computing the mel spectrograms\n",
    "        spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n",
    "        spect = librosa.power_to_db(spect, ref=np.max)\n",
    "        \n",
    "        # Adjusting the size to be 128 x 660\n",
    "        if spect.shape[1] != 660:\n",
    "            spect.resize(128,660, refcheck=False)\n",
    "            \n",
    "        # Adding the mel spectrogram to the list\n",
    "        mel_specs.append(spect)\n",
    "        \n",
    "    # Converting the list or arrays to an array\n",
    "    X = np.array(mel_specs)\n",
    "    \n",
    "    # Converting labels to numeric values\n",
    "    labels = pd.Series(labels)\n",
    "    label_dict = {\n",
    "        'jazz': 0,\n",
    "        'reggae': 1,\n",
    "        'rock': 2,\n",
    "        'blues': 3,\n",
    "        'hiphop': 4,\n",
    "        'country': 5,\n",
    "        'metal': 6,\n",
    "        'classical': 7,\n",
    "        'disco': 8,\n",
    "        'pop': 9\n",
    "    }\n",
    "    y = labels.map(label_dict).values\n",
    "    \n",
    "    # Returning the mel spectrograms and labels\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function to read and extract mel spectrograms from the GTZAN Genre Dataset audio files\n",
    "X, y = extract_mel_spectrogram('../data/wavfiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It is important that we train test split before scaling and reshaping the data to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the minimum value (the scale ranges from zero to some negative value) to see how we should scale the data\n",
    "X_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling our data to be between 0 and 1 using the minimum value from above\n",
    "X_train /= -80\n",
    "X_test /= -80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping images to be 128 x 660 x 1, where the 1 represents the single color channel\n",
    "X_train = X_train.reshape(X_train.shape[0], 128, 660, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 128, 660, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding our labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Feed Forward Neural Network (FFNN)\n",
    "Before running a CNN, I wanted to train a feed forward neural network (FFNN) for comparison. CNNs have additional layers for edge detection that make them well suited for image classification problems, but they tend to be more computationally expensive than FFNNs. If a FFNN could perform just as well, there would be no need to use a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing a random seed for replication purposes\n",
    "np.random.seed(23456)\n",
    "\n",
    "# Initiating an empty neural network\n",
    "model = Sequential()\n",
    "\n",
    "# Adding a flattened layer to input our image data\n",
    "model.add(Flatten(input_shape = (128, 660, 1)))\n",
    "\n",
    "# Adding a dense layer with 128 neurons\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Adding a dense layer with 128 neurons\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Adding a dense layer with 64 neurons\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Adding an output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compiling our neural network\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fitting our neural network\n",
    "history = model.fit(X_train,\n",
    "                    y_train, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** After trying several different architectures, the best model (based on test accuracy) achieved a training score of 69% and a test score of 45%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a random seed for replication purposes\n",
    "np.random.seed(23456)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Initiating an empty neural network\n",
    "cnn_model = Sequential(name='cnn_1')\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model.add(Conv2D(filters=16,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(128,660,1)))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model.add(Conv2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding a flattened layer to input our image data\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# Adding a dense layer with 64 neurons\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Adding a dropout layer for regularization\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "# Adding an output layer\n",
    "cnn_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compiling our neural network\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Fitting our neural network\n",
    "history = cnn_model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the model summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The code in this cell was adapted from a lecture at General Assembly\n",
    "\n",
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(test_loss, label='Testing Loss', color='red')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(1,16), range(1,16))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The code in this cell was adapted from a lecture at General Assembly\n",
    "\n",
    "# Check out our train accuracy and test accuracy over epochs.\n",
    "train_loss = history.history['accuracy']\n",
    "test_loss = history.history['val_accuracy']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Accuracy', color='blue')\n",
    "plt.plot(test_loss, label='Testing Accuracy', color='red')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Accuracy by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Accuracy', fontsize = 18)\n",
    "plt.xticks(range(1,21), range(1,21))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Most of the models I ran became increasingly overfit after about 15-20 epochs, so increasing the number of epochs would likely not improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions from the cnn model\n",
    "predictions = cnn_model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "To look deeper into what was happening with the model, I computed a confusion matrix to visualize the model's predictions against the actual values.\n",
    "\n",
    "**Note:** since the confusion matrix function from sklearn does not return the labels for predicted values and actual values, I checked how many predicted and actual values there were for each genre to be able to figure it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of targets per class\n",
    "for i in range(10): \n",
    "    print(f'{i}: {sum([1 for target in y_test if target[i] == 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the number of predicted values in each class\n",
    "for i in range(10): \n",
    "    print(f'{i}: {sum([1 for prediction in predictions if np.argmax(prediction) == i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix \n",
    "# row: actual\n",
    "# columns: predicted\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, 1), np.argmax(predictions, 1))\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe of the confusion matrix with labels for readability \n",
    "confusion_df = pd.DataFrame(conf_matrix)\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of labels\n",
    "labels_dict = {\n",
    "    0: 'jazz',\n",
    "    1: 'reggae',\n",
    "    2: 'rock',\n",
    "    3: 'blues',\n",
    "    4: 'hiphop',\n",
    "    5: 'country',\n",
    "    6: 'metal',\n",
    "    7: 'classical',\n",
    "    8: 'disco',\n",
    "    9: 'pop'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming rows and columns with labes\n",
    "confusion_df = confusion_df.rename(columns=labels_dict)\n",
    "confusion_df.index = confusion_df.columns\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a heatmap for the confusion matrix for display\n",
    "plt.figure(figsize= (20,12))\n",
    "sns.set(font_scale = 2);\n",
    "ax = sns.heatmap(confusion_df, annot=True, cmap=sns.cubehelix_palette(50));\n",
    "ax.set(xlabel='Predicted Values', ylabel='Actual Values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The best CNN model (based on test score accuracy) achieved a score of 68%. The training score was 84%, so the model was overfit. This means that it was tuning really well to the training data and not generalizing as well to new data. Even so, it's certainly learning.\n",
    "\n",
    "I tried several different architectures to try to improve the model, and most of them achieved accuracies between 55 and 65 percent, but I couldn't get it much above that. Most of the models became increasingly overfit after about 15-20 epochs, so increasing the number of epochs likely would not improve the model.\n",
    "\n",
    "Based on the confusion matrix, the computer was confusing some genres for others similarly to how humans would. For example, most of the misclassifications for blues was either jazz or rock. This makes sense because blues heavily influenced rock music, and shares several characteristics with both rock and jazz. Raggea and hiphop were also mixed up, which makes sense because hiphop was influenced by reggae. \n",
    "\n",
    "This is actually really good news! Our model is running into the same difficulties that a human would. It's clearly learning some of the distinguishing factors of the musical genres, but it is having trouble with genres that share characteristics with other genres. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
